# Nube Backend

Backend en Flask para la plataforma Nube, encargado de exponer servicios REST relacionados con calidad del aire, clima y anal√≠tica derivada de datos abiertos. El proyecto incluye utilidades para descargar datasets de OpenAQ, Meteostat/OpenWeather y OpenStreetMap, as√≠ como notebooks y scripts de modelado predictivo.

## Tecnolog√≠as principales
- Python 3.13
- Flask + Flask-CORS
- Requests y utilidades propias para integrar APIs externas
- Prophet, CatBoost y TensorFlow para modelos de series de tiempo y regresi√≥n
- Docker para empaquetado y despliegue

## Estructura del repositorio
```
backend/
‚îú‚îÄ‚îÄ Nube.py                 # Punto de entrada Flask
‚îú‚îÄ‚îÄ api/                    # Blueprint principal y conectores externos
‚îÇ   ‚îú‚îÄ‚îÄ routes.py           # Endpoints (actualmente placeholders)
‚îÇ   ‚îú‚îÄ‚îÄ constants.py        # IDs de sensores y claves esperadas via .env
‚îÇ   ‚îú‚îÄ‚îÄ requests_service.py # Cliente HTTP b√°sico con autenticaci√≥n OpenAQ
‚îÇ   ‚îú‚îÄ‚îÄ openaq_connection.py
‚îÇ   ‚îú‚îÄ‚îÄ weather_api_connection.py
‚îÇ   ‚îî‚îÄ‚îÄ openstreetmap_connection.py
‚îú‚îÄ‚îÄ ai/                     # Scripts y datos para modelos predictivos
‚îÇ   ‚îú‚îÄ‚îÄ model.py            # Pipeline de entrenamiento NN + CatBoost
‚îÇ   ‚îî‚îÄ‚îÄ predict_data.py     # Pron√≥sticos con Prophet y an√°lisis estacional
‚îú‚îÄ‚îÄ cache/                  # Ejemplos de resultados cacheados (JSON)
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ requirements.txt
```
> Nota: las carpetas `ai/air_data`, `ai/weather_data` y `ai/traffic_data` contienen datasets comprimidos (.csv.gz) que alimentan los experimentos de ML.

## Requisitos previos
- Python 3.13 (o 3.11+ si se ajustan dependencias de TensorFlow)
- pip y virtualenv
- Opcional: Docker 24+

## Variables de entorno
Def√≠nelas en un archivo `.env` en la ra√≠z o exporta en tu shell antes de ejecutar la app.
- `OPEN_AQ_KEY`: clave de API de OpenAQ (requerida para llamadas autenticadas).
- `TEMPO_KEY`: token para la API TEMPO (reservado para integraci√≥n futura).
- `EARTH_ACCESS_KEY`: credencial para fuentes de EarthData (verifica `constants.py`, actualmente usa `os.get` y deber√° corregirse a `os.getenv`).
- `PORT`: puerto de escucha; por defecto `5000`.

Ejemplo de `.env`:
```
OPEN_AQ_KEY=tu_token
TEMPO_KEY=otro_token
EARTH_ACCESS_KEY=credencial
PORT=5000
```

## Instalaci√≥n y ejecuci√≥n local
1. Crear entorno virtual
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```
2. Instalar dependencias
   ```bash
   pip install -r requirements.txt
   ```
3. Levantar el servidor Flask
   ```bash
   python Nube.py
   ```
4. Abre `http://localhost:5000/api/health` para verificar el estado.

## Ejecuci√≥n con Docker
1. Construir la imagen
   ```bash
   docker build -t nube-backend .
   ```
2. Ejecutar el contenedor (mapeando el puerto que necesites)
   ```bash
   docker run --env-file .env -p 5000:5000 nube-backend
   ```

## Endpoints actuales (`api/routes.py`)
| M√©todo | Ruta               | Estado | Descripci√≥n |
|--------|--------------------|--------|-------------|
| GET    | `/api/health`      | ‚úÖ     | Health check simple. |
| GET    | `/api/aq/latest`   | üöß     | Devolver √∫ltima medici√≥n de PM2.5/NO2 y clima; pendiente integrar clientes. |
| GET    | `/api/aq/trends`   | üöß     | Tendencias hist√≥ricas y correlaciones. |
| GET    | `/api/aq/forecast` | üöß     | Pron√≥stico 24h usando Prophet/ARIMA. |
| GET    | `/api/aq/sources`  | üöß     | Metadatos de datasets usados. |
| POST   | `/api/alerts/subscribe` | üöß | Registro de preferencias para alertas. |

> Los endpoints marcados como üöß contienen TODOs para conectar con los clientes reales (`tempo_client`, `openaq_client`, `weather_client`) y persistir la informaci√≥n.

## Scripts y utilidades
- `api/openaq_connection.py`: descarga datos hist√≥ricos de sensores regiomontanos desde el bucket S3 de OpenAQ.
- `api/weather_api_connection.py`: baja series horarias de Meteostat para estaciones cercanas.
- `api/openstreetmap_connection.py`: genera un dataset sint√©tico de tr√°fico para Monterrey usando OSMnx.
- `ai/predict_data.py`: pron√≥sticos y visualizaciones (Prophet, resampling diario).
- `ai/model.py`: pipeline de entrenamiento para gases y part√≠culas (NN + CatBoost).
- `ai/test.py`: consulta industrias cercanas a un sensor v√≠a Overpass API (diagn√≥stico).

### Datos usados por los modelos (`ai/`)
- `ai/create_dataset.py` fusiona todas las fuentes necesarias para entrenar los modelos:
  - **Calidad del aire**: agrega todas las estaciones presentes en `ai/air_data/` (Universidad, Tecnol√≥gico, Centro de Monterrey, San Nicol√°s, Escobedo, Apodaca). Cada CSV se pivota por `parameter`, se normalizan columnas (`location_id`, `value`) y se ordena por estaci√≥n.
  - **Clima**: combina todas las estaciones Meteostat en `ai/weather_data/`, calcula medias horarias (`temp_mean`, `rhum_mean`, `wspd_mean`) y preserva las columnas individuales (`temp_76393`, `temp_MMMY0`, etc.).
  - **Ingenier√≠a de atributos**: interpola huecos cortos (hasta 6 muestras), genera rolling averages y diferencias (6/24 h) para cada contaminante, adem√°s de variables de calendario (hora, d√≠a de la semana, mes, `sin/cos`).
- El dataset unificado se guarda en `ai/output/dataset_final.csv` y contiene todas las estaciones de la ZMM con `location_id`, `lat`, `lon` y las features generadas.
- `ai/model.py` consume ese dataset consolidado: el modelo de gases usa todas las variables meteorol√≥gicas y las series de contaminantes, mientras que los modelos CatBoost de part√≠culas se entrenan por objetivo pero siguen viendo los datos de todas las estaciones (s√≥lo descartan filas que no tengan el contaminante espec√≠fico).
- `ai/models/` almacena los artefactos (`gas_model.keras`, `catboost_pm10.cbm`, `catboost_pm25.cbm`) junto con metadatos de columnas e imputaciones para reproducir inferencia en API (`/api/aq/predictions`).

## Flujo de trabajo recomendado
1. **Sincronizar datos**: ejecutar los scripts de descarga seg√∫n sea necesario (ocupando claves y verificando cuotas de API).
2. **Entrenar modelos**: correr `ai/model.py` o `ai/predict_data.py` para actualizar modelos y pron√≥sticos.
3. **Exponer servicios**: completar los TODOs en `api/routes.py` conectando los clientes de datos y modelos.
4. **Despliegue**: usar el Dockerfile y Railway/Render para desplegar el backend (asegura las variables de entorno).

## Pr√≥ximos pasos sugeridos
- Implementar los clientes `tempo_client`, `openaq_client` y `weather_client` y conectarlos en los endpoints.
- A√±adir almacenamiento para suscripciones (por ejemplo, base de datos ligera o servicio externo).
- Sustituir prints por logging estructurado y manejar errores HTTP con c√≥digos apropiados.
- Automatizar pruebas b√°sicas de los endpoints (PyTest + Flask testing client).
